{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.7.223:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training data.\n",
    "base_df = spark.read.option(\"header\", \"true\").csv(\"s3a://ids-2017-group42/train_ver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required import libraries.\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values\n",
    "names = base_df.schema.names\n",
    "null_count = []\n",
    "for col_x in names:\n",
    "    null_count.append(base_df.where(col(col_x).isNull()).count())\n",
    "    \n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_df.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_time='2014-07-16 10:00:06', site_name='2', posa_continent='3', user_location_country='66', user_location_region='189', user_location_city='10067', orig_destination_distance=None, user_id='501', is_mobile='0', is_package='0', channel='2', srch_ci='2014-08-01', srch_co='2014-08-02', srch_adults_cnt='2', srch_children_cnt='0', srch_rm_cnt='1', srch_destination_id='8267', srch_destination_type_id='1', is_booking='0', cnt='1', hotel_continent='2', hotel_country='50', hotel_market='675', hotel_cluster='98')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering out columns where checking and checkout dates are null\n",
    "base_df = base_df.where(col('srch_ci').isNotNull()).where(col('srch_co').isNotNull())\n",
    "# base_df = base_df\n",
    "base_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting date columns to relevant data formats.\n",
    "conv_to_date = udf(lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())\n",
    "\n",
    "base_df = base_df.withColumn(\"srch_ci\", conv_to_date(col('srch_ci'))).withColumn(\"srch_co\", conv_to_date(col('srch_co')))\n",
    "# base_df = base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# getting a list of columns in base_df to convert to appropriate data type.\n",
    "col_names = base_df.schema.names\n",
    "# removing date_time column.\n",
    "col_names.remove('date_time')\n",
    "\n",
    "base_df = base_df.select(*col_names)\n",
    "print(len(col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting all data to int type.\n",
    "col_names.remove('srch_ci')\n",
    "col_names.remove('srch_co')\n",
    "\n",
    "# casting columns to int.\n",
    "for names in col_names:\n",
    "    base_df = base_df.withColumn(names, base_df[names].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move all the column additions here, calculate all new columns before scaling.\n",
    "# Adding Season column for applying filtering.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def returnSeason(x):\n",
    "    if x in [1, 2, 12]:\n",
    "        return \"0\"\n",
    "    elif x in [3, 4, 5]:\n",
    "        return \"1\"\n",
    "    elif x in [6, 7, 8]:\n",
    "        return \"2\"\n",
    "    elif x in [9, 10, 11]:\n",
    "        return \"3\"\n",
    "    \n",
    "season_udf = udf(returnSeason, \"int\")\n",
    "\n",
    "# adding column to total the number of people in the booking.\n",
    "def addPeople(x1, x2):\n",
    "    return x1+x2\n",
    "\n",
    "udf_people = udf(addPeople, \"int\")\n",
    "\n",
    "# adding columns to the dataframe.\n",
    "base_df = base_df.withColumn('travel_season', season_udf(F.month(col('srch_ci')))) \\\n",
    "            .withColumn(\"no_persons\", udf_people(col('srch_adults_cnt'), col('srch_children_cnt'))) \\\n",
    "            .withColumn(\"days_stayed\", F.datediff(col('srch_co'), col('srch_ci')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# dont need to scale the following columns.\n",
    "col_names.remove('orig_destination_distance')\n",
    "col_names.remove('hotel_cluster')\n",
    "col_names.remove('user_id')\n",
    "col_names.remove('is_mobile')\n",
    "col_names.remove('is_booking')\n",
    "\n",
    "def scaleColumns(base_df, col_name):\n",
    "    # creating vector for column.\n",
    "    col_feature = col_name + \"_feature\"\n",
    "    assembler = VectorAssembler(\n",
    "            inputCols=[col_name],\n",
    "            outputCol=col_feature)\n",
    "    assembled = assembler.transform(base_df)\n",
    "    \n",
    "    col_scaled = col_name + \"_scaled\"\n",
    "    # creating scaled rows for the column.\n",
    "    scaler = MinMaxScaler(inputCol=col_feature, outputCol=col_scaled)\n",
    "    scalerModel = scaler.fit(assembled)\n",
    "    base_df = scalerModel.transform(assembled)\n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"dcb8199e-62f7-4a73-aaa3-4da05b7fc25b\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"dcb8199e-62f7-4a73-aaa3-4da05b7fc25b\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scaling all the columns.\n",
    "for col_name in col_names:\n",
    "    base_df = scaleColumns(base_df, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regression_assembler = VectorAssembler(inputCols=['posa_continent_scaled', 'user_location_country_scaled', 'user_location_region_scaled', 'user_location_city_scaled', 'hotel_continent_scaled', 'hotel_country_scaled', 'hotel_market_scaled'],\n",
    "                                      outputCol='regression_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_df = regression_assembler.transform(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regression_df = base_df.where(base_df['orig_destination_distance'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]\n"
     ]
    }
   ],
   "source": [
    "# search space for all the regularization parameters.\n",
    "regParam1 = [i/10 for i in range(1,10)]\n",
    "regParam2 = [i/100 for i in range(1,10)]\n",
    "regParam_range = regParam1 + regParam2\n",
    "print(regParam_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# linear regression model.\n",
    "lr = LinearRegression(featuresCol=\"regression_features\", labelCol=\"orig_destination_distance\", maxIter=10, predictionCol=\"predicted_distance\")\n",
    "\n",
    "# constructing ml pipeline to check the best possible parameters.\n",
    "pipeline1 = Pipeline(stages=[lr])\n",
    "\n",
    "# construction paramGrid object to choose the best regularization parameters.\n",
    "paramGrib = ParamGridBuilder().addGrid(lr.regParam,regParam_range).build()\n",
    "\n",
    "# construction regression evaluator.\n",
    "regEvaluator = RegressionEvaluator(predictionCol=\"predicted_distance\", labelCol=\"orig_destination_distance\", metricName=\"r2\")\n",
    "\n",
    "# cross validator.\n",
    "crossVal = CrossValidator(estimator=pipeline1, estimatorParamMaps=paramGrib, evaluator=regEvaluator, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_40d7a98cd89e1ee017dc\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"a0837433-724d-46b7-81f9-124395b583e4\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"a0837433-724d-46b7-81f9-124395b583e4\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 672 ms, total: 2.27 s\n",
      "Wall time: 2h 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%notify\n",
    "# fit the cross validation model to regression_df.\n",
    "regModel = crossVal.fit(regression_df)\n",
    "print(regModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the best model from the cross validator model.\n",
    "regPipeline = regModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model trained for further use.\n",
    "regPipeline.stages[0].save(\"LinearRegressionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading the model, if it is trained.\n",
    "regModel1 = LinearRegressionModel.load(\"LinearRegressionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# doing regression on the data.\n",
    "base_df = regModel1.transform(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# filtering df to evaluate error values.\n",
    "base_df_reg = base_df.where(base_df['orig_destination_distance'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# new regression model -> Random Forest Regression.\n",
    "rf_model = RandomForestRegressor(featuresCol=\"regression_features\", labelCol=\"orig_destination_distance\", predictionCol=\"predicted_distance_rf\")\n",
    "\n",
    "# creating a pipeline for random forest regression.\n",
    "pipeline2 = Pipeline(stages=[rf_model])\n",
    "# pipeline2 = rf_model\n",
    "\n",
    "# building parameters for random forest to choose from.\n",
    "numTrees_range = [i for i in range(2,3)] # increase these parameters when training on the bigger instance.\n",
    "maxDepth_range = [i for i in range(5,6)] # increase these parameters when training on the bigger instance.\n",
    "\n",
    "# construction paramGrid object to choose the best parameters for random forest.\n",
    "paramGrid2 = ParamGridBuilder().addGrid(rf_model.numTrees,numTrees_range).addGrid(rf_model.maxDepth, maxDepth_range).build()\n",
    "\n",
    "# construction regression evaluator.\n",
    "regEvaluator2 = RegressionEvaluator(predictionCol=\"predicted_distance_rf\", labelCol=\"orig_destination_distance\", metricName=\"r2\")\n",
    "\n",
    "# cross validator.\n",
    "crossVal2 = CrossValidator(estimator=pipeline2, estimatorParamMaps=paramGrid2, evaluator=regEvaluator2, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"cbd65b80-e072-404a-98cb-ba5c1a76dda6\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"cbd65b80-e072-404a-98cb-ba5c1a76dda6\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 20 ms, total: 148 ms\n",
      "Wall time: 7min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%notify\n",
    "# fit the cross validation model to regression_df.\n",
    "rf_model = crossVal2.fit(regression_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the Random forest model.\n",
    "regPipeline2 = rf_model.bestModel\n",
    "\n",
    "# saving the best model trained for further use.\n",
    "regPipeline2.stages[0].save(\"RandomForestModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# load from memory\n",
    "rf_model1 = RandomForestRegressionModel.load(\"RandomForestModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_df_rf = rf_model1.transform(base_df)\n",
    "base_df_rf = base_df_rf.where(base_df_rf['orig_destination_distance'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7012087821315416\n"
     ]
    }
   ],
   "source": [
    "# print the model evaluation results. - random forest model.\n",
    "print(regEvaluator2.evaluate(base_df_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "row5_df_assembler = VectorAssembler(\n",
    "        inputCols=['posa_continent_scaled','site_name_scaled','user_location_country_scaled','user_location_region_scaled','user_location_city_scaled', 'user_id'],\n",
    "        outputCol=\"kmeans_features\")\n",
    "\n",
    "# apply filtering to reduce get unique values of user features using drop duplicates.\n",
    "clustering_df = row5_df_assembler.transform(base_df) \\\n",
    "    .select(['user_id', 'kmeans_features']) \\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=9681, kmeans_features=DenseVector([0.25, 0.2157, 0.1925, 0.4012, 0.6253, 9681.0]))]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to choose a good value for clustering.\n",
    "k_value = base_df.select('user_location_region').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"4eff381b-d2d5-450c-96e8-ebcc43a7bdda\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"4eff381b-d2d5-450c-96e8-ebcc43a7bdda\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# kmeans model.\n",
    "kmeans = KMeans(featuresCol=\"kmeans_features\", predictionCol=\"predicted_clusters\",initMode=\"k-means||\", initSteps=2,\n",
    "                tol=1e-4,maxIter=20, k=15000, seed=27)\n",
    "kmodel = kmeans.fit(clustering_df)\n",
    "wsse = kmodel.computeCost(clustering_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- site_name: integer (nullable = true)\n",
      " |-- posa_continent: integer (nullable = true)\n",
      " |-- user_location_country: integer (nullable = true)\n",
      " |-- user_location_region: integer (nullable = true)\n",
      " |-- user_location_city: integer (nullable = true)\n",
      " |-- orig_destination_distance: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- is_mobile: integer (nullable = true)\n",
      " |-- is_package: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- srch_ci: date (nullable = true)\n",
      " |-- srch_co: date (nullable = true)\n",
      " |-- srch_adults_cnt: integer (nullable = true)\n",
      " |-- srch_children_cnt: integer (nullable = true)\n",
      " |-- srch_rm_cnt: integer (nullable = true)\n",
      " |-- srch_destination_id: integer (nullable = true)\n",
      " |-- srch_destination_type_id: integer (nullable = true)\n",
      " |-- is_booking: integer (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      " |-- hotel_continent: integer (nullable = true)\n",
      " |-- hotel_country: integer (nullable = true)\n",
      " |-- hotel_market: integer (nullable = true)\n",
      " |-- hotel_cluster: integer (nullable = true)\n",
      " |-- travel_season: integer (nullable = true)\n",
      " |-- no_persons: integer (nullable = true)\n",
      " |-- days_stayed: integer (nullable = true)\n",
      " |-- site_name_feature: vector (nullable = true)\n",
      " |-- site_name_scaled: vector (nullable = true)\n",
      " |-- posa_continent_feature: vector (nullable = true)\n",
      " |-- posa_continent_scaled: vector (nullable = true)\n",
      " |-- user_location_country_feature: vector (nullable = true)\n",
      " |-- user_location_country_scaled: vector (nullable = true)\n",
      " |-- user_location_region_feature: vector (nullable = true)\n",
      " |-- user_location_region_scaled: vector (nullable = true)\n",
      " |-- user_location_city_feature: vector (nullable = true)\n",
      " |-- user_location_city_scaled: vector (nullable = true)\n",
      " |-- is_package_feature: vector (nullable = true)\n",
      " |-- is_package_scaled: vector (nullable = true)\n",
      " |-- channel_feature: vector (nullable = true)\n",
      " |-- channel_scaled: vector (nullable = true)\n",
      " |-- srch_adults_cnt_feature: vector (nullable = true)\n",
      " |-- srch_adults_cnt_scaled: vector (nullable = true)\n",
      " |-- srch_children_cnt_feature: vector (nullable = true)\n",
      " |-- srch_children_cnt_scaled: vector (nullable = true)\n",
      " |-- srch_rm_cnt_feature: vector (nullable = true)\n",
      " |-- srch_rm_cnt_scaled: vector (nullable = true)\n",
      " |-- srch_destination_id_feature: vector (nullable = true)\n",
      " |-- srch_destination_id_scaled: vector (nullable = true)\n",
      " |-- srch_destination_type_id_feature: vector (nullable = true)\n",
      " |-- srch_destination_type_id_scaled: vector (nullable = true)\n",
      " |-- cnt_feature: vector (nullable = true)\n",
      " |-- cnt_scaled: vector (nullable = true)\n",
      " |-- hotel_continent_feature: vector (nullable = true)\n",
      " |-- hotel_continent_scaled: vector (nullable = true)\n",
      " |-- hotel_country_feature: vector (nullable = true)\n",
      " |-- hotel_country_scaled: vector (nullable = true)\n",
      " |-- hotel_market_feature: vector (nullable = true)\n",
      " |-- hotel_market_scaled: vector (nullable = true)\n",
      " |-- regression_features: vector (nullable = true)\n",
      " |-- predicted_distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to dataframe to rdd, ml library doesnt have ALS factorization method.\n",
    "new_df = base_df.select('user_id', 'hotel_cluster', 'predicted_distance', 'is_booking', 'cnt', 'travel_season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = scaleColumns(new_df, 'predicted_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting the selected df to rdd.\n",
    "recommendation_rdd = new_df.select('user_id', 'hotel_cluster', 'predicted_distance_scaled', 'is_booking', 'cnt').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((501, 98), (0.36599233403023351, (0, 1)))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping the rdd to key-value pairs.\n",
    "recommendation_rdd = recommendation_rdd.map(lambda x: ((x[0], x[1]) , (x[2].values[0], x[3:])))\n",
    "recommendation_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Transforming the rdd to use as input for colloborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((501, 98), (0.36599233403023351, 0, 1))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation_rdd = recommendation_rdd.map(lambda x: (x[0], (x[1][0], x[1][1][0], x[1][1][1])))\n",
    "recommendation_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_dict = recommendation_rdd.map(lambda x: x[0]).countByValue()\n",
    "count_broadcast = sc.broadcast(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((92893, 17), (0.40782858329157284, 0, 1))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_rdd_1 = recommendation_rdd.reduceByKey(lambda x, y: x+y)\n",
    "rec_rdd_1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying that the transformation take place.\n",
    "rec_rdd_2 = rec_rdd_1.filter(lambda x: len(x[1]) > 3 and len(x[1])%3 != 0)\n",
    "rec_rdd_2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineRatings(x):\n",
    "    if len(x[1]) > 3:\n",
    "        length = len(x[1])\n",
    "        index = 0\n",
    "        count = 0\n",
    "        dist, is_b, srch_cnt = 0, 1, 0\n",
    "        while length != index:\n",
    "            dist += x[1][index]\n",
    "            index += 1\n",
    "            is_b *= x[1][index]\n",
    "            index += 1\n",
    "            srch_cnt += x[1][index]\n",
    "            index += 1\n",
    "            count += 1\n",
    "        return (x[0], (1 - (dist/count), is_b, srch_cnt))\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((92893, 17), (0.40782858329157284, 0, 1))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_rdd_3 = rec_rdd_1.map(combineRatings)\n",
    "rec_rdd_3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1679826"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_rdd_2 = rec_rdd_3.filter(lambda x: len(x[1]) == 3)\n",
    "rec_rdd_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_rdd4 = rec_rdd_3.map(lambda x: (x[0], x[1], count_broadcast.value[x[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((92893, 17), (0.40782858329157284, 0, 1, 1))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_rdd5 = rec_rdd4.map(lambda x: (x[0], (x[1][0], x[1][1], x[1][2], x[2])))\n",
    "rec_rdd5.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((92893, 17), 0.81565716658314569)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_rdd = rec_rdd5.map(lambda x: (x[0], x[1][0] * (x[1][1] + x[1][2] + x[1][3])))\n",
    "ratings_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=92893, product=17, rating=0.8156571665831457)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "\n",
    "ratings_rdd = ratings_rdd.map(lambda x: Rating(x[0][0], x[0][1], x[1]))\n",
    "ratings_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
      "[10, 12, 14, 16, 18, 20]\n",
      "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank_list = [i for i in range(10, 55, 5)]\n",
    "num_list = [i for i in range(10, 22, 2)]\n",
    "alpha_list = [i/10 for i in range(1, 10)]\n",
    "model_list = {}\n",
    "error_list = {}\n",
    "\n",
    "print(rank_list)\n",
    "print(num_list)\n",
    "print(alpha_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings_rdd\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# recommendation model -> used for finding out the best hyper parameters.\n",
    "# for rank in rank_list:\n",
    "#     for numIterations in num_list:\n",
    "#         for a in alpha_list:\n",
    "#             rec_model = ALS.trainImplicit(ratings_rdd, rank, numIterations, alpha=a)\n",
    "#             x = str(rank) + \"-\" + str(numIterations) + \"-\" + str(a) + \"-model\"\n",
    "            \n",
    "#             predictions = rec_model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "#             ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "#             MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "#             error_list[x] = MSE\n",
    "#             del rec_model\n",
    "\n",
    "# once optimal hyperparameters were known, we used those to train further models.\n",
    "rec_model = ALS.trainImplicit(ratings_rdd, 10, 12, alpha=0.01)\n",
    "predictions = rec_model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings_rdd.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32202371917\n"
     ]
    }
   ],
   "source": [
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the recommendation model.\n",
    "rec_model.save(sc, \"RecommendationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model back, no need to retrain the model.\n",
    "rec_model1 = MatrixFactorizationModel.load(sc, \"RecommendationModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading test data.\n",
    "test_df = spark.read.option(\"header\", \"true\").csv(\"s3a://ids-2017-group42/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_time: string (nullable = true)\n",
      " |-- site_name: string (nullable = true)\n",
      " |-- posa_continent: string (nullable = true)\n",
      " |-- user_location_country: string (nullable = true)\n",
      " |-- user_location_region: string (nullable = true)\n",
      " |-- user_location_city: string (nullable = true)\n",
      " |-- orig_destination_distance: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- is_mobile: string (nullable = true)\n",
      " |-- is_package: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- srch_ci: string (nullable = true)\n",
      " |-- srch_co: string (nullable = true)\n",
      " |-- srch_adults_cnt: string (nullable = true)\n",
      " |-- srch_children_cnt: string (nullable = true)\n",
      " |-- srch_rm_cnt: string (nullable = true)\n",
      " |-- srch_destination_id: string (nullable = true)\n",
      " |-- srch_destination_type_id: string (nullable = true)\n",
      " |-- hotel_continent: string (nullable = true)\n",
      " |-- hotel_country: string (nullable = true)\n",
      " |-- hotel_market: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying same preprocessing transformation that were applied to base df.\n",
    "test_df = test_df.withColumn(\"srch_ci\", conv_to_date(col('srch_ci'))).withColumn(\"srch_co\", conv_to_date(col('srch_co')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'site_name', 'posa_continent', 'user_location_country', 'user_location_region', 'user_location_city', 'orig_destination_distance', 'user_id', 'is_mobile', 'is_package', 'channel', 'srch_ci', 'srch_co', 'srch_adults_cnt', 'srch_children_cnt', 'srch_rm_cnt', 'srch_destination_id', 'srch_destination_type_id', 'hotel_continent', 'hotel_country', 'hotel_market']\n"
     ]
    }
   ],
   "source": [
    "col_names = test_df.schema.names\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names.remove('date_time')\n",
    "# # names_df.remove('BookingDate')\n",
    "test_df = test_df.select(*col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names.remove('srch_ci')\n",
    "col_names.remove('srch_co')\n",
    "\n",
    "for names in col_names:\n",
    "    test_df = test_df.withColumn(names, test_df[names].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn('travel_season', season_udf(F.month(col('srch_ci')))) \\\n",
    "    .withColumn(\"no_persons\", udf_people(col('srch_adults_cnt'), col('srch_children_cnt'))) \\\n",
    "    .withColumn(\"days_stayed\", F.datediff(col('srch_co'), col('srch_ci')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont need to scale the following columns.\n",
    "col_names.remove('orig_destination_distance')\n",
    "col_names.remove('user_id')\n",
    "col_names.remove('is_mobile')\n",
    "col_names.remove('is_booking')\n",
    "\n",
    "def scaleColumns(base_df, col_name):\n",
    "    # creating vector for column.\n",
    "    col_feature = col_name + \"_feature\"\n",
    "    assembler = VectorAssembler(\n",
    "            inputCols=[col_name],\n",
    "            outputCol=col_feature)\n",
    "    assembled = assembler.transform(base_df)\n",
    "    \n",
    "    col_scaled = col_name + \"_scaled\"\n",
    "    # creating scaled rows for the column.\n",
    "    scaler = MinMaxScaler(inputCol=col_feature, outputCol=col_scaled)\n",
    "    scalerModel = scaler.fit(assembled)\n",
    "    base_df = scalerModel.transform(assembled)\n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"7f9a0a24-b8ab-4f3f-9492-9828f9b4ae63\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"7f9a0a24-b8ab-4f3f-9492-9828f9b4ae63\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# scaling all the columns.\n",
    "for col_name in col_names:\n",
    "    test_df = scaleColumns(test_df, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transformations for running regression.\n",
    "test_df = regression_assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the pretrained model to estimate test distances.\n",
    "test_df = regModel1.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = test_df.select('id','user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_ratings = new_test_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating recommendation for all users.\n",
    "rec_ratings = rec_model1.recommendProductsForUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(318784, (41, 5, 70, 25, 98))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_ratings1 = rec_ratings.map(lambda x: (x[0], (x[1][0].product, x[1][1].product, x[1][2].product, x[1][3].product, x[1][4].product)))\n",
    "rec_ratings1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_rdd = eval_ratings.join(rec_ratings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd.filter(lambda x: x==None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, (21, 98, 18, 95, 70))]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd = final_rdd.map(lambda x: (x[0], x[1][0], x[1][1]))\n",
    "final_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rdd = final_rdd.map(lambda x: (x[1], x[0], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0, (21, 98, 18, 95, 70))]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id, user_id, cluster recommendation.\n",
    "final_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying all users have recommendation.\n",
    "final_rdd_n.filter(lambda x: x[2] == None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_rdd = final_rdd_n.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '21 98 18 95 70')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd1 = final_rdd.map(lambda x: (x[1], ' '.join(str(d) for d in x[2])))\n",
    "final_rdd1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '21 98 18 95 70')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd2 = final_rdd1.map(lambda x: (str(x[0]), x[1]))\n",
    "final_rdd2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,21 98 18 95 70']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd3 = final_rdd2.map(lambda x: ','.join(i for i in x))\n",
    "final_rdd3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '21 98 18 95 70')]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_df = final_rdd2.toDF(['id', 'hotel_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='0', hotel_cluster='21 98 18 95 70')]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df.coalesce(1).write.option(\"header\",\"true\").csv('s3a://ids-2017-group42/result1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## >>>end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
